{
  "metadata": {
    "timestamp_utc": "2025-08-09T01:02:25Z",
    "batch_size_eval": 32,
    "candidate_k_eval": 50,
    "experiment_prefix": "QUERY-REWRITE_rerank_bm25",
    "retrieval_top_k": 5,
    "candidate_k": 50,
    "query_rewriter": {
      "strategy_flags": {
        "enable_cce": true,
        "enable_kwr": true,
        "enable_gqr": true,
        "enable_prf": false,
        "enable_decompose": true
      },
      "expansion_terms": 3,
      "lang": "italian",
      "device": "cpu",
      "base_retriever": {
        "type": "rerank_bm25",
        "bm25": {
          "use_stopwords": true,
          "use_stemming": true,
          "k1": 1.5,
          "b": 0.75,
          "corpus": {
            "num_chunks": 801,
            "min_len": 4,
            "max_len": 2687,
            "avg_len": 91.82521847690387
          }
        },
        "reranker": {
          "reranker_model": "cross-encoder/ms-marco-MiniLM-L-6-v2",
          "device": "cpu",
          "max_length": 256,
          "batch_size": 64,
          "max_rerank_candidates": 20
        }
      },
      "reranker": {
        "reranker_model": "cross-encoder/ms-marco-MiniLM-L-6-v2",
        "device": "cpu",
        "max_length": 256,
        "batch_size": 64,
        "max_rerank_candidates": 20
      }
    },
    "dataset": "combined",
    "combined_sources": [
      "singlehop",
      "multihop"
    ],
    "shuffle": true
  },
  "metrics": {
    "num_queries": 908,
    "avg_precision@k": 0.10814977973568281,
    "avg_recall@k": 0.35187224669603523,
    "avg_mrr": 0.30339574155653454,
    "avg_ndcg@k": 0.35684612672920385,
    "avg_ap@k": 0.20598231767009298,
    "avg_retrieval_time": 4.965474704405886
  }
}